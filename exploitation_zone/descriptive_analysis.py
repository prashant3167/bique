import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, explode, year, month, to_date, when, dayofmonth
from pyspark.sql.types import DoubleType
from  daily_analysis import daily_analysis_initializer
import argparse
from datetime import datetime


# Set the HDFS URL
hdfs_url = "hdfs://10.4.41.51:27000/user/bdm"

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Descriptive analysis") \
    .getOrCreate()


def weekly_analysis(data):
    """
    Perform weekly spending analysis on the given DataFrame.

    Args:
        data (DataFrame): The input DataFrame containing the transaction data.

    Returns:
        None. The analysis results are written to HDFS.
    """
    monthly_distribution = data.select(col("fullDocument_month")).first()[0]
    yearly_distribution = data.select(col("fullDocument_year")).first()[0]

    data = data.withColumn(
        "monthweek",
        when((dayofmonth(col("fullDocument_date")) >= 1) & (dayofmonth(col("fullDocument_date")) <= 7), 1)
        .when((dayofmonth(col("fullDocument_date")) >= 8) & (dayofmonth(col("fullDocument_date")) <= 14), 2)
        .when((dayofmonth(col("fullDocument_date")) >= 15) & (dayofmonth(col("fullDocument_date")) <= 21), 3)
        .when((dayofmonth(col("fullDocument_date")) >= 22) & (dayofmonth(col("fullDocument_date")) <= 28), 4)
        .otherwise(5)
    )

    # Weekly spending analysis
    week_df = data.filter(col("fullDocument_transactionAmount_amount") <= 0).groupBy(
        "fullDocument_source", "fullDocument_year", "fullDocument_month", "monthweek"
    ).agg(sum("fullDocument_transactionAmount_amount").alias("weekanalysis"))

    week_path = f'{hdfs_url}/exploited_zone/aggregations/weekanalysis/dt={yearly_distribution}/mon={monthly_distribution}'
    week_df.write.mode("overwrite").partitionBy('fullDocument_source').parquet(week_path)


def monthly_analysis(data):
    """
    Perform monthly spending analysis on the given DataFrame.

    Args:
        data (DataFrame): The input DataFrame containing the transaction data.

    Returns:
        None. The analysis results are written to HDFS.
    """
    # Analysis 3: Transactions per year and month
    monthly_analysis_df = data.groupBy(
        "fullDocument_source", "fullDocument_year", "fullDocument_month"
    ).agg(sum("fullDocument_transactionAmount_amount").alias("TotalAmountByYearMonth"))

    hdfs_path_mamount = f'{hdfs_url}/exploited_zone/aggregations/monthYearAmount'
    monthly_analysis_df.write.mode("append").partitionBy('fullDocument_source').parquet(hdfs_path_mamount)

    # Analysis 4: Categories transactions per year and month
    category_month_analysis = data.groupBy(
        "fullDocument_source", "fullDocument_year", "fullDocument_month", "transactionCategory"
    ).agg(sum("fullDocument_transactionAmount_amount").alias("TotalAmountByYearMonth"))

    hdfs_path = f'{hdfs_url}/exploited_zone/aggregations/monthYearCategoryAmount'
    category_month_analysis.write.mode("append").partitionBy('fullDocument_source').parquet(hdfs_path)

    # Analysis 4: Income per month
    income_path = f'{hdfs_url}/exploited_zone/aggregations/monthlyincome'
    income_df = data.filter(col("fullDocument_transactionAmount_amount") > 0)
    income_df_group = income_df.groupBy(
        "fullDocument_source", "fullDocument_year", "fullDocument_month"
    ).agg(sum("fullDocument_transactionAmount_amount").alias("income"))
    income_df_group.write.mode("append").partitionBy('fullDocument_source').parquet(income_path)

    # Analysis 4: Expenditure per month
    spent_path = f'{hdfs_url}/exploited_zone/aggregations/monthlyspent'
    spent_df = data.filter(col("fullDocument_transactionAmount_amount") <= 0)
    spent_df_group = spent_df.groupBy(
        "fullDocument_source", "fullDocument_year", "fullDocument_month"
    ).agg(sum("fullDocument_transactionAmount_amount").alias("spent"))
    spent_df_group.write.mode("append").partitionBy('fullDocument_source').parquet(spent_path)


def perform_analysis(analysis_type):
    # Read data from Parquet files
    data = spark.read.parquet(f'{hdfs_url}/formatted_data/bique.transactions/')

    # Convert column to DoubleType
    data = data.withColumn("fullDocument_transactionAmount_amount", col("fullDocument_transactionAmount_amount").cast(DoubleType()))

    # Convert date column to date type
    data = data.withColumn("fullDocument_date", to_date("fullDocument_date"))
    data = data.withColumn("fullDocument_year", year("fullDocument_date"))
    data = data.withColumn("fullDocument_month", month("fullDocument_date"))

    # Explode transactionCategory column
    data = data.withColumn("transactionCategory", explode("fullDocument_transactionInformation"))

    current_date = datetime.now()
    last_date = datetime.now() - timedelta(days=1)

    if analysis_type == "monthly":
        data = data.filter(
            (col("fullDocument_year") == last_date.year) & (col("fullDocument_month") == last_date.month)
        )
        monthly_analysis(data)
    elif analysis_type == "weekly":
        data = data.filter(
            (col("fullDocument_year") == current_date.year) & (col("fullDocument_month") == current_date.month)
        )
        weekly_analysis(data)
    elif analysis_type == "daily":
        daily_analysis_initializer(spark = spark,hdfs_url = hdfs_url)
        


if __name__ == '__main__':
    # Set up the argument parser
    parser = argparse.ArgumentParser(description='Perform weekly spending analysis for a specified month.')
    parser.add_argument('type', type=str, help='Type of Report')

    # Parse the command-line arguments
    args = parser.parse_args()
    analysis_type = args.type

    # Call the function to perform the analysis
    perform_analysis(analysis_type)