import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, count, explode, year, month, to_date, when, dayofmonth
from pyspark.sql.types import DoubleType
spark = SparkSession.builder \
    .appName("Write to HDFS") \
    .getOrCreate()

data = spark.read.parquet("hdfs://10.4.41.51:27000/user/bdm/formatted_data/bique.transactions/")

# data transformations
data = data.withColumn("fullDocument_transactionAmount_amount",
                       col("fullDocument_transactionAmount_amount").cast(DoubleType()))

data = data.withColumn("transactionCategory", explode("fullDocument_transactionInformation"))

# Convert the date column to date type and extracting month and year
data = data.withColumn("fullDocument_date", to_date("fullDocument_date"))
data = data.withColumn("fullDocument_year", year("fullDocument_date"))
data = data.withColumn("fullDocument_month", month("fullDocument_date"))
data = data.withColumn("monthweek",
    when((col("fullDocument_date").day >= 0) & (col("fullDocument_date").day <= 7), 1)
    .when((col("fullDocument_date").day >= 8) & (col("fullDocument_date").day <= 14), 2)
    .when((col("fullDocument_date").day >= 15) & (col("fullDocument_date").day <= 21), 3)
    .when((col("fullDocument_date").day >= 22) & (col("fullDocument_date").day <= 28), 4)
    .otherwise(5)
)

# analysis 1: transactions
accountData = data.groupBy("fullDocument_source").agg(sum("fullDocument_transactionAmount_amount").alias("TotalAmount"))

# analysis 2: categories
categoryData = data.groupBy("fullDocument_source","transactionCategory").agg(sum("fullDocument_transactionAmount_amount").alias("TotalAmount"))

# analysis 3: transactions per year and month
acc_df = data.filter(.groupBy("fullDocument_source","fullDocument_year","fullDocument_month").agg(sum("fullDocument_transactionAmount_amount").alias("TotalAmountByYearMonth"))

# analysis 4: categories transactions per year and month
acc_cat_df = data.groupBy("fullDocument_source","fullDocument_year","fullDocument_month","transactionCategory").agg(sum("fullDocument_transactionAmount_amount").alias("TotalAmountByYearMonth"))
hdfs_path = f'hdfs://10.4.41.51:27000/user/bdm/exploited_zone/aggregations/monthYearCategoryAmount'
acc_cat_df.write.mode("append").partitionBy('fullDocument_source').parquet(hdfs_path)
                     
# Weekly spending analysis
week_df = data.filter(col("fullDocument_transactionAmount_amount") <= 0).groupBy("fullDocument_source","fullDocument_year","fullDocument_month", "monthweek").agg(sum("fullDocument_transactionAmount_amount").alias("weekanalysis"))
week_path = f'hdfs://10.4.41.51:27000/user/bdm/exploited_zone/aggregations/weekanalysis'
week_df.write.mode("overwrite").partitionBy('fullDocument_source').parquet(week_path)

  

                       
 # analysis 4: Income per month                       
income_path = f'hdfs://10.4.41.51:27000/user/bdm/exploited_zone/aggregations/monthtlyincome'

income_df = data.filter(col("fullDocument_transactionAmount_amount") > 0)
income_df_group = income_df.groupBy("fullDocument_source","fullDocument_year","fullDocument_month").agg(sum("fullDocument_transactionAmount_amount").alias("income"))
income_df_group.write.mode("append").partitionBy('fullDocument_source').parquet(income_path)

# analysis 4: Expenditure per month
spent_path = f'hdfs://10.4.41.51:27000/user/bdm/exploited_zone/aggregations/monthtlyspent'

spent_df = data.filter(col("fullDocument_transactionAmount_amount") <= 0)
spent_df_group = spent_df.groupBy("fullDocument_source","fullDocument_year","fullDocument_month").agg(sum("fullDocument_transactionAmount_amount").alias("spent"))
spent_df_group.write.mode("append").partitionBy('fullDocument_source').parquet(spent_path)

